{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1YXSXX8QXVqDNVZETLr7moBdRXRZAKBS-","authorship_tag":"ABX9TyPwwkMrGMS3Mn2j1IZstdqm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"12hen4N5X1hK"},"outputs":[],"source":["!pip install vit-keras"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, Input\n","from tensorflow.keras.layers import Activation, Concatenate, Conv2D, Multiply\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","import tensorflow_addons as tfa\n","import glob, warnings\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","from vit_keras import vit\n","\n","warnings.filterwarnings('ignore')\n","print('TensorFlow Version ' + tf.__version__)\n","\n","train=r'/content/drive/MyDrive/Dataset with Code/Train'\n","test=r'/content/drive/MyDrive/Dataset with Code/Test'\n","val=r'/content/drive/MyDrive/Dataset with Code/Val'\n","\n","train_datagen=tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n","                                                              samplewise_center = True,\n","                                                              samplewise_std_normalization = True)\n","val_datagen=tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n","                                                            samplewise_center = True,\n","                                                            samplewise_std_normalization = True)\n","test_datagen=tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n","                                                             samplewise_center = True,\n","                                                             samplewise_std_normalization = True)\n","\n","train_ds = train_datagen.flow_from_directory(train,\n","                                          target_size = (224,224),batch_size = 8,\n","                                         shuffle = True,seed = 42)\n","test_ds = test_datagen.flow_from_directory(test,\n","                                          target_size = (224,224),batch_size = 8,\n","                                         shuffle = False,seed = 42)\n","val_ds = val_datagen.flow_from_directory(val,\n","                                          target_size = (224,224),batch_size = 8,\n","                                         shuffle = False,seed = 42)\n","\n","from tensorflow.python.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.models import Model\n","\n","vit_model = vit.vit_b16(\n","        image_size = 224,\n","        activation = 'softmax',\n","        pretrained = True,\n","        include_top = False,\n","        pretrained_top = False,\n","        classes = 39)\n","\n","IMAGE_SIZE = 224\n","initializer = tf.keras.initializers.GlorotNormal()\n","\n","inputs = Input(shape=(224, 224, 3))\n","x = vit_model.output\n","x = Flatten()(x)\n","x = tf.keras.layers.BatchNormalization()(x)\n","x = Dropout(0.4)(x)\n","x = Dense(128, activation='relu', kernel_initializer=initializer)(x)\n","outputs = Dense(39, activation='softmax')(x)\n","model = Model(inputs = vit_model.input, outputs = outputs, name='vision_transformer_cbam')\n","#model.summary()\n","\n","early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=3,\n","    restore_best_weights=True)\n","\n","# Define Reduce Learning Rate Callback\n","reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_loss',\n","    patience=2,\n","    factor=0.1,\n","    verbose=1)\n","\n","# Define Callbacks and Metrics lists\n","CALLBACKS = [early_stopping_callback, reduce_lr_callback]\n","METRICS = ['accuracy']\n","\n","learning_rate = 1e-4\n","#Initially set with 0.01\n","# After patience = 5 of val_loss, factor = 0.1\n","\n","optimizer = tfa.optimizers.RectifiedAdam(learning_rate=learning_rate)\n","\n","model.compile(optimizer=optimizer,\n","              loss=tf.keras.losses.CategoricalCrossentropy(), # Remove label_smoothing\n","              metrics=['accuracy'])\n","\n","history = model.fit(\n","    train_ds,\n","    batch_size=8,\n","    epochs=10,\n","    validation_data=val_ds,\n","    callbacks=[early_stopping_callback, reduce_lr_callback]\n",")"],"metadata":{"id":"SzG80FiDMW2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores = model.evaluate(test_ds)"],"metadata":{"id":"W5OLRYMBMXCQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","class_labels = list(test_ds.class_indices.keys())\n","y_pred1 = model.predict(test_ds)\n","y_pred1 = np.argmax(y_pred1, axis=1)\n","\n","from sklearn.metrics import classification_report\n","print(classification_report(test_ds.classes, y_pred1, target_names=class_labels))"],"metadata":{"id":"qvMCqVDOMXKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# Make predictions on the test data\n","predictions = model.predict(test_ds)\n","\n","# Convert predictions and true labels into class indices\n","predicted_classes = np.argmax(predictions, axis=1)\n","true_classes = test_ds.classes\n","\n","# Get class labels from the generator\n","class_labels = list(test_ds.class_indices.keys())\n","\n","# Calculate accuracy for each class\n","class_accuracies = {}\n","for i, label in enumerate(class_labels):\n","    class_indices = np.where(true_classes == i)[0]\n","    class_predictions = predicted_classes[class_indices]\n","    class_true_labels = true_classes[class_indices]\n","    class_accuracy = accuracy_score(class_true_labels, class_predictions)\n","    class_accuracies[label] = class_accuracy\n","\n","# Print accuracy for each class\n","for label, accuracy in class_accuracies.items():\n","    print(f\"Accuracy for class {label}: {accuracy}\")"],"metadata":{"id":"LxkoiCC2MXg5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"rxgeTwCsMXtH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = test_ds.classes\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","# Assuming labels contain the class names\n","labels = test_ds.class_indices.keys()\n","\n","plt.figure(figsize=(18, 18))\n","hm = sns.heatmap(confusion_matrix(y_true, y_pred1), annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False,\n","            xticklabels=labels, yticklabels=labels, color='green')\n","hm.set(xlabel='Predicted_labels')\n","hm.set(ylabel='True_labels')\n","plt.show()"],"metadata":{"id":"OXAx-4HnMX5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = test_ds.classes\n","\n","from sklearn.metrics import confusion_matrix\n","\n","cm = confusion_matrix(y_true, y_pred1)\n","\n","num_classes = cm.shape[0]\n","\n","specificities = []\n","for i in range(num_classes):\n","    tn = cm[i, i]\n","    fp = cm.sum(axis=0)[i] - tn\n","    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n","    specificities.append(specificity)\n","\n","class_names = test_ds.class_indices.keys()  # Assuming test_data is your test dataset\n","\n","print(\"Specificity per class:\")\n","for class_name, class_specificity in zip(class_names, specificities):\n","    print(f\"{class_name}: {class_specificity:.2f}\")\n","\n","class_weights = np.bincount(y_true) / len(y_true)\n","\n","weighted_specificity = np.average(specificities, weights=class_weights)\n","print(\"Weighted Averaged Specificity:\", weighted_specificity)\n","\n","macro_specificity = sum(specificities) / len(specificities)\n","print(\"Macro-Averaged Specificity:\", macro_specificity)"],"metadata":{"id":"pk78J9pGNAiO"},"execution_count":null,"outputs":[]}]}